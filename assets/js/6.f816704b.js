(window.webpackJsonp=window.webpackJsonp||[]).push([[6],{352:function(l,t,o){"use strict";o.r(t);var _=o(42),s=Object(_.a)({},(function(){var l=this,t=l.$createElement,o=l._self._c||t;return o("ContentSlotsDistributor",{attrs:{"slot-key":l.$parent.slotKey}},[o("h1",{attrs:{id:"logistic回归（分类问题）"}},[o("a",{staticClass:"header-anchor",attrs:{href:"#logistic回归（分类问题）"}},[l._v("#")]),l._v(" Logistic回归（分类问题）")]),l._v(" "),o("hr"),l._v(" "),o("ol",[o("li",[l._v("二分类任务")])]),l._v(" "),o("blockquote",[o("ul",[o("li",[l._v("输出标记$y\\in{0,1}$，即将一组输入的特征值识别为0类或1类。")])])]),l._v(" "),o("ol",{attrs:{start:"2"}},[o("li",[l._v("sigmoid函数（logistic函数）")])]),l._v(" "),o("blockquote",[o("ul",[o("li",[l._v("将实数映射到$(0,1)$范围。")]),l._v(" "),o("li",[l._v("$f:R\\rightarrow (0,1),f(z)=\\frac{1}{1+e^{-z}}$")]),l._v(" "),o("li",[l._v("将线性回归预测值映射到sigmoid函数上即可进行二分类任务：\n$$\nh_\\theta(\\boldsymbol{x})=f(\\boldsymbol{\\theta}^T\\boldsymbol{x})=\\frac{1}{1+e^{-\\boldsymbol{\\theta}^T\\boldsymbol{x}}}\n$$")]),l._v(" "),o("li",[l._v("其意义为在$\\boldsymbol{\\theta},\\boldsymbol{x}$给定的情况下，$y=1$的概率：\n$$\nh_\\theta(\\boldsymbol{x})=P(y=1|\\boldsymbol{x};\\boldsymbol{\\theta})\n$$")])])]),l._v(" "),o("ol",{attrs:{start:"3"}},[o("li",[l._v("决策边界")])]),l._v(" "),o("blockquote",[o("ul",[o("li",[l._v("即分类的界限")]),l._v(" "),o("li",[l._v("取$h_\\theta(\\boldsymbol{x})=0.5$可得决策边界：$\\boldsymbol{\\theta}^T\\boldsymbol{x}=0$")])])]),l._v(" "),o("ol",{attrs:{start:"3"}},[o("li",[l._v("代价函数")])]),l._v(" "),o("blockquote",[o("ul",[o("li",[l._v("如果使用与线性回归相同的代价函数，即$J(\\boldsymbol{\\theta})=\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(\\boldsymbol{x}^{(i)})-y^{(i)})^2$，由于该函数并不是凸函数，使用梯度下降难以找到最优参数。")]),l._v(" "),o("li",[l._v("可以使用另一种代价函数代替。首先定义单组样本的代价：\n$$\ncost(h_\\theta(\\boldsymbol{x}),y)=\\begin{cases}-\\log(h_\\theta(\\boldsymbol{x}))&,y=1\\-\\log(1-h_\\theta(\\boldsymbol{x}))&,y=0\\end{cases}\n$$")]),l._v(" "),o("li",[l._v("改写为另一种形式：\n$$\ncost(h_\\theta(\\boldsymbol{x}),y)=-(y\\cdot\\log(h_\\theta(\\boldsymbol{x}))+(1-y)\\cdot\\log(1-h_\\theta(\\boldsymbol{x})))\n$$")]),l._v(" "),o("li",[l._v("则logistic回归的代价函数定义为：\n$$\nJ(\\boldsymbol{\\theta})=\\frac{1}{m}\\sum_{i=1}^mcost(h_\\theta(\\boldsymbol{x}^{(i)}), y^{(i)})\\newline=-\\frac{1}{m}\\sum_{i=1}^m[y^{(i)}\\log(h_\\theta(\\boldsymbol{x^{(i)}}))+(1-y^{(i)})\\log(1-h_\\theta(\\boldsymbol{x^{(i)}}))]\n$$")])])]),l._v(" "),o("ol",{attrs:{start:"4"}},[o("li",[l._v("梯度下降法")])]),l._v(" "),o("blockquote",[o("ul",[o("li",[l._v("目标：$\\min\\limits_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})$")]),l._v(" "),o("li",[l._v("迭代方法：$\\theta_j^{(k+1)}:=\\theta_j^{(k)}-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\boldsymbol{\\theta}^{(k)})$")]),l._v(" "),o("li",[l._v("将$J(\\boldsymbol{\\theta})$代入可得：$\\theta_j^{(k+1)}:=\\theta_j^{(k)}-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(\\boldsymbol{x^{(i)}})-y^{(i)})x^{(i)}_j$（其中k是当前迭代轮数）")]),l._v(" "),o("li",[l._v("该迭代公式与线性回归的迭代公式相同")])])]),l._v(" "),o("ol",{attrs:{start:"5"}},[o("li",[l._v("高级优化方法")])]),l._v(" "),o("blockquote",[o("ul",[o("li",[l._v("共轭梯度法（Conjugate Gradient）")]),l._v(" "),o("li",[l._v("BFGS")]),l._v(" "),o("li",[l._v("L-BFGS")])])])])}),[],!1,null,null,null);t.default=s.exports}}]);