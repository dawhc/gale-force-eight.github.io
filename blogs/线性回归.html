<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>线性回归 | Gale Force Eight&#39;s Blog</title>
    <meta name="generator" content="VuePress 1.5.2">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
    <meta name="description" content="">
    <link rel="preload" href="/assets/css/0.styles.d6754e65.css" as="style"><link rel="preload" href="/assets/js/app.79a0865e.js" as="script"><link rel="preload" href="/assets/js/2.6b553fcd.js" as="script"><link rel="preload" href="/assets/js/8.82a85830.js" as="script"><link rel="prefetch" href="/assets/js/3.4cfc6a47.js"><link rel="prefetch" href="/assets/js/4.0bfcab90.js"><link rel="prefetch" href="/assets/js/5.819bb03b.js"><link rel="prefetch" href="/assets/js/6.f816704b.js"><link rel="prefetch" href="/assets/js/7.edd96109.js">
    <link rel="stylesheet" href="/assets/css/0.styles.d6754e65.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Gale Force Eight's Blog</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blogs.html" class="nav-link">
  机器学习
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/blogs.html" class="nav-link">
  机器学习
</a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="线性回归"><a href="#线性回归" class="header-anchor">#</a> 线性回归</h1> <hr> <ol><li>线性回归模型的基本形式</li></ol> <blockquote><ul><li>单元：$h_\theta(x)=\theta_0+\theta_1x$</li> <li>多元：$h_\theta(\boldsymbol{x})=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n=\boldsymbol{\theta}^T\boldsymbol{x}$</li></ul></blockquote> <ol start="2"><li>代价函数</li></ol> <blockquote><ul><li>用于评价一组参数的拟合程度的好坏</li> <li>$J(\boldsymbol{\theta})=\frac{1}{2m}\sum_{i=1}^m(h_\theta(\boldsymbol{x}^{(i)})-y^{(i)})^2$</li></ul></blockquote> <ol start="3"><li>梯度下降法</li></ol> <blockquote><ul><li>每次令$\theta_j$向当前代价函数$J(\boldsymbol{\theta})$梯度值下降的方向移动一段距离</li> <li>迭代方法：$\theta_j^{(k+1)}:=\theta_j^{(k)}-\alpha\frac{\partial}{\partial\theta_j}J(\boldsymbol{\theta}^{(k)})$</li> <li>将$J(\boldsymbol{\theta})$代入可得：$\theta_j^{(k+1)}:=\theta_j^{(k)}-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(\boldsymbol{x^{(i)}})-y^{(i)})x^{(i)}_j$（其中k是当前迭代轮数）</li></ul></blockquote> <ol start="4"><li>学习率$\alpha$的选择</li></ol> <blockquote><ul><li>$\alpha$太低导致梯度下降的收敛速度变慢，迭代时间变长；$\alpha$太高导致梯度下降的幅度太大以致难以收敛</li> <li>$\alpha$的选择可以按照3倍等比数列进行测试，如$0.03,0.1,0.3,1,3,10,30,100$等。</li></ul></blockquote> <ol start="5"><li>特征缩放</li></ol> <blockquote><ul><li>如果特征值$x_j$之间的规模（分度值）差距比较大，可能会导致梯度下降的速度变慢，此时可以将特征值进行缩放处理</li> <li>处理方法：$x_j:=\frac{x_j-\mu_j}{s_j}$，其中$\mu_j$可以选择均值，$s_j$可以选择$max{x}-min{x}$或者$x_j$的标准差</li></ul></blockquote> <ol start="6"><li>正规方程法</li></ol> <blockquote><ul><li>本质其实就是最小二乘法</li> <li>求解公式为$\boldsymbol{\theta}=(X^TX)^{-1}X^T\boldsymbol{y}$，其中$X=\begin{bmatrix}(\boldsymbol{x}^{(1)})^T\(\boldsymbol{x}^{(2)})^T\(\boldsymbol{x}^{(3)})^T\\vdots\(\boldsymbol{x}^{(m)})^T\end{bmatrix},\boldsymbol{x}^{(i)}=\begin{bmatrix}x_0^{(i)}\x_1^{(i)}\x_2^{(i)}\\vdots\x_n^{(i)}\end{bmatrix},\boldsymbol{y}=\begin{bmatrix}y^{(1)}\y^{(2)}\y^{(3)}\\vdots\y^{(m)}\end{bmatrix}$</li> <li>如果$X^TX$的逆矩阵不存在，可以使用伪逆矩阵。</li></ul></blockquote> <ol start="7"><li>多项式回归方法</li></ol> <blockquote><ul><li>假设多项式回归模型为$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3$，可以令$x_i:=x^i$，进而采用上述方法。</li></ul></blockquote></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.79a0865e.js" defer></script><script src="/assets/js/2.6b553fcd.js" defer></script><script src="/assets/js/8.82a85830.js" defer></script>
  </body>
</html>
