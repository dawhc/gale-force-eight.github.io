(window.webpackJsonp=window.webpackJsonp||[]).push([[7],{353:function(t,a,e){"use strict";e.r(a);var _=e(42),h=Object(_.a)({},(function(){var t=this,a=t.$createElement,e=t._self._c||a;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"机器学习-梯度下降法"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#机器学习-梯度下降法"}},[t._v("#")]),t._v(" 机器学习——梯度下降法")]),t._v(" "),e("hr"),t._v(" "),e("ol",[e("li",[t._v("方法原理")])]),t._v(" "),e("blockquote",[e("p",[t._v("一种寻找最优解的方法，通过迭代实现")]),t._v(" "),e("p",[t._v("设$\\theta=[\\theta_j]$为参数(或者参数矩阵)，$J(\\theta)$为代价函数")]),t._v(" "),e("p",[t._v("则目标为$\\min\\limits_\\theta{J(\\theta)}$")]),t._v(" "),e("p",[t._v("梯度下降法即每次沿着$J(\\theta)$的梯度下降的方向更新参数$\\theta$，使得$J(\\theta)$的值不断下降直至局部最优。")]),t._v(" "),e("p",[t._v("迭代公式为：$\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)$")]),t._v(" "),e("p",[t._v("优点：容易实现；n很大时效率优秀")]),t._v(" "),e("p",[t._v("缺点：需要进行迭代；需要选取合适的学习率$\\alpha$；容易陷入局部最优解而非全局最优解")])]),t._v(" "),e("ol",{attrs:{start:"2"}},[e("li",[t._v("算法伪代码")])]),t._v(" "),e("blockquote",[e("p",[t._v("$repeat\\ \\ until\\ \\ convergence{\\\\ \\ \\ \\ for\\ \\ \\theta_j\\ \\ in\\ \\ \\theta:\\\\ \\ \\ \\ \\ \\ \\ \\ temp_j=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta)\\\\ \\ \\ \\ for\\ \\ \\theta_j\\ \\ in\\ \\ \\theta:\\\\ \\ \\ \\ \\ \\ \\ \\ \\theta_j = temp_j\\}$")])])])}),[],!1,null,null,null);a.default=h.exports}}]);