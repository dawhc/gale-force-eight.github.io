(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{354:function(t,l,_){"use strict";_.r(l);var a=_(42),o=Object(a.a)({},(function(){var t=this,l=t.$createElement,_=t._self._c||l;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("h1",{attrs:{id:"线性回归"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#线性回归"}},[t._v("#")]),t._v(" 线性回归")]),t._v(" "),_("hr"),t._v(" "),_("ol",[_("li",[t._v("线性回归模型的基本形式")])]),t._v(" "),_("blockquote",[_("ul",[_("li",[t._v("单元：$h_\\theta(x)=\\theta_0+\\theta_1x$")]),t._v(" "),_("li",[t._v("多元：$h_\\theta(\\boldsymbol{x})=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n=\\boldsymbol{\\theta}^T\\boldsymbol{x}$")])])]),t._v(" "),_("ol",{attrs:{start:"2"}},[_("li",[t._v("代价函数")])]),t._v(" "),_("blockquote",[_("ul",[_("li",[t._v("用于评价一组参数的拟合程度的好坏")]),t._v(" "),_("li",[t._v("$J(\\boldsymbol{\\theta})=\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(\\boldsymbol{x}^{(i)})-y^{(i)})^2$")])])]),t._v(" "),_("ol",{attrs:{start:"3"}},[_("li",[t._v("梯度下降法")])]),t._v(" "),_("blockquote",[_("ul",[_("li",[t._v("每次令$\\theta_j$向当前代价函数$J(\\boldsymbol{\\theta})$梯度值下降的方向移动一段距离")]),t._v(" "),_("li",[t._v("迭代方法：$\\theta_j^{(k+1)}:=\\theta_j^{(k)}-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\boldsymbol{\\theta}^{(k)})$")]),t._v(" "),_("li",[t._v("将$J(\\boldsymbol{\\theta})$代入可得：$\\theta_j^{(k+1)}:=\\theta_j^{(k)}-\\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(\\boldsymbol{x^{(i)}})-y^{(i)})x^{(i)}_j$（其中k是当前迭代轮数）")])])]),t._v(" "),_("ol",{attrs:{start:"4"}},[_("li",[t._v("学习率$\\alpha$的选择")])]),t._v(" "),_("blockquote",[_("ul",[_("li",[t._v("$\\alpha$太低导致梯度下降的收敛速度变慢，迭代时间变长；$\\alpha$太高导致梯度下降的幅度太大以致难以收敛")]),t._v(" "),_("li",[t._v("$\\alpha$的选择可以按照3倍等比数列进行测试，如$0.03,0.1,0.3,1,3,10,30,100$等。")])])]),t._v(" "),_("ol",{attrs:{start:"5"}},[_("li",[t._v("特征缩放")])]),t._v(" "),_("blockquote",[_("ul",[_("li",[t._v("如果特征值$x_j$之间的规模（分度值）差距比较大，可能会导致梯度下降的速度变慢，此时可以将特征值进行缩放处理")]),t._v(" "),_("li",[t._v("处理方法：$x_j:=\\frac{x_j-\\mu_j}{s_j}$，其中$\\mu_j$可以选择均值，$s_j$可以选择$max{x}-min{x}$或者$x_j$的标准差")])])]),t._v(" "),_("ol",{attrs:{start:"6"}},[_("li",[t._v("正规方程法")])]),t._v(" "),_("blockquote",[_("ul",[_("li",[t._v("本质其实就是最小二乘法")]),t._v(" "),_("li",[t._v("求解公式为$\\boldsymbol{\\theta}=(X^TX)^{-1}X^T\\boldsymbol{y}$，其中$X=\\begin{bmatrix}(\\boldsymbol{x}^{(1)})^T\\(\\boldsymbol{x}^{(2)})^T\\(\\boldsymbol{x}^{(3)})^T\\\\vdots\\(\\boldsymbol{x}^{(m)})^T\\end{bmatrix},\\boldsymbol{x}^{(i)}=\\begin{bmatrix}x_0^{(i)}\\x_1^{(i)}\\x_2^{(i)}\\\\vdots\\x_n^{(i)}\\end{bmatrix},\\boldsymbol{y}=\\begin{bmatrix}y^{(1)}\\y^{(2)}\\y^{(3)}\\\\vdots\\y^{(m)}\\end{bmatrix}$")]),t._v(" "),_("li",[t._v("如果$X^TX$的逆矩阵不存在，可以使用伪逆矩阵。")])])]),t._v(" "),_("ol",{attrs:{start:"7"}},[_("li",[t._v("多项式回归方法")])]),t._v(" "),_("blockquote",[_("ul",[_("li",[t._v("假设多项式回归模型为$h_\\theta(x)=\\theta_0+\\theta_1x+\\theta_2x^2+\\theta_3x^3$，可以令$x_i:=x^i$，进而采用上述方法。")])])])])}),[],!1,null,null,null);l.default=o.exports}}]);